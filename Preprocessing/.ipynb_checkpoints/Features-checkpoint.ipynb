{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install pyphen\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import emoji\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter, OrderedDict, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyphen\n",
    "PYPHEN_DIC = pyphen.Pyphen(lang='en')\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train, dev, test sets\n",
    "This step crucial here for ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_test_dirs(root_directory, output_directory):\n",
    "    if os.path.exists(root_directory+'/'+output_directory+\"/train\"):\n",
    "        shutil.rmtree(root_directory+'/'+output_directory+\"/train/\")\n",
    "        shutil.rmtree(root_directory+'/'+output_directory+\"/dev/\")\n",
    "        shutil.rmtree(root_directory+'/'+output_directory+\"/test/\")\n",
    "\n",
    "    os.makedirs(root_directory+'/'+output_directory+\"/train\")\n",
    "    os.makedirs(root_directory+'/'+output_directory+\"/dev\")\n",
    "    os.makedirs(root_directory+'/'+output_directory+\"/test\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(root_directory, input_directory, output_directory):\n",
    "    clean_input_directory = input_directory + '_clean'\n",
    "    clean_output_directory = output_directory + '_clean'\n",
    "    make_train_test_dirs(root_directory, output_directory)\n",
    "    make_train_test_dirs(root_directory, clean_output_directory)\n",
    "    \n",
    "    Truth = pd.read_csv(root_directory+'/'+input_directory+'/truth.txt', sep=\":::\", header=None, engine='python')\n",
    "    Truth_train = open(root_directory+'/'+output_directory+'/truth_train.txt', 'w')\n",
    "    Truth_dev = open(root_directory+'/'+output_directory+'/truth_dev.txt', 'w')\n",
    "    Truth_test = open(root_directory+'/'+output_directory+'/truth_test.txt', 'w')\n",
    "    \n",
    "    # dicitionary indicating whether a twitter profile (xml) is in train, dev or test set\n",
    "    roleDict = {}\n",
    "    for i in range(0,Truth.shape[0]):\n",
    "        role = np.random.rand()\n",
    "        text = ''\n",
    "        TweetGenerator = LoadProfile(root_directory+'/'+input_directory+'/'+Truth[0][i]+'.xml')\n",
    "        for tweet in TweetGenerator:\n",
    "            text += tweet['data']\n",
    "        if role <= 0.7:\n",
    "            roleDict[Truth[0][i]] = 'train'\n",
    "            Truth_train.write(Truth[0][i]+':::'+Truth[1][i]+'\\n')\n",
    "            copyfile(root_directory+'/'+clean_input_directory+'/'+Truth[0][i]+'.txt', root_directory+'/'+clean_output_directory+\"/train\"+'/'+Truth[0][i]+'.txt')\n",
    "            copyfile(root_directory+'/'+input_directory+'/'+Truth[0][i]+'.xml', root_directory+'/'+output_directory+\"/train\"+'/'+Truth[0][i]+'.xml')\n",
    "        elif role <= 0.8:\n",
    "            roleDict[Truth[0][i]] = 'dev'\n",
    "            Truth_dev.write(Truth[0][i]+':::'+Truth[1][i]+'\\n')\n",
    "            copyfile(root_directory+'/'+clean_input_directory+'/'+Truth[0][i]+'.txt', root_directory+'/'+clean_output_directory+\"/dev\"+'/'+Truth[0][i]+'.txt')\n",
    "            copyfile(root_directory+'/'+input_directory+'/'+Truth[0][i]+'.xml', root_directory+'/'+output_directory+\"/dev\"+'/'+Truth[0][i]+'.xml')\n",
    "        else:\n",
    "            roleDict[Truth[0][i]] = 'test'\n",
    "            Truth_test.write(Truth[0][i]+':::'+Truth[1][i]+'\\n')\n",
    "            copyfile(root_directory+'/'+clean_input_directory+'/'+Truth[0][i]+'.txt', root_directory+'/'+clean_output_directory+\"/test\"+'/'+Truth[0][i]+'.txt')\n",
    "            copyfile(root_directory+'/'+input_directory+'/'+Truth[0][i]+'.xml', root_directory+'/'+output_directory+\"/test\"+'/'+Truth[0][i]+'.xml')\n",
    "    Truth_train.close()\n",
    "    Truth_dev.close()\n",
    "    Truth_test.close()\n",
    "    return roleDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roleDict = train_test_split('C:\\\\Users\\\\Vanda\\\\Documents\\\\PAN19\\\\data\\\\pan19-author-profiling-training-2019-01-28', 'en', 'en_split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nGram features\n",
    "Build an nGram model based on train set, then transform dev and test sets according to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nGrams_model():\n",
    "    print(\"Getting texts...\")\n",
    "    train_text = get_text('C:\\\\Users\\\\pedalo\\\\Documents\\\\GithubCodes\\\\PAN-bot-detection\\\\pan19-author-profiling-training-2019-01-28', 'en_split', 'train')\n",
    "    tfidf = TfidfVectorizer(min_df=5, max_df=0.5, max_features = 200, ngram_range=(2,2))\n",
    "    print(\"Fitting vectorizer...\")\n",
    "    train_features = tfidf.fit_transform(train_text)\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    return tfidf, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regexDigits = re.compile('^[0-9]*$')\n",
    "def get_text(root_directory, input_directory, mode):\n",
    "    clean_input_directory = input_directory + '_clean'\n",
    "    Truth = pd.read_csv(root_directory+'/'+input_directory+'/truth_'+mode+'.txt', sep=\":::\", header=None, engine='python')\n",
    "    text = []\n",
    "    for i in range(0,Truth.shape[0]):\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "        tweet_text = ''\n",
    "        file = open(root_directory+'/'+clean_input_directory+'/'+mode+'/'+Truth[0][i]+'.txt', 'r', encoding='utf-8')\n",
    "        for line in file:\n",
    "            # substitute digits\n",
    "            line = re.sub('\\\\d+', '0', line)\n",
    "            line = lemmatizeLine(line)\n",
    "            tweet_text += line\n",
    "        file.close()\n",
    "        text.append(tweet_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatizeLine(line):\n",
    "    line = spacy_nlp(line)\n",
    "    newLine = []\n",
    "    for token in line:\n",
    "        lemma = token.lemma_\n",
    "        newLine.append(lemma)\n",
    "#         if lemma not in spacy_stopwords:\n",
    "#             newLine.append(lemma)\n",
    "    newLine = ' '.join(newLine)\n",
    "    return newLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_features, dev_features, test_features, feature_names = nGrams()\n",
    "ngram_model, ngram_features = nGrams_model()\n",
    "import pickle\n",
    "pickle.dump(ngram_model, open('C:\\\\Users\\\\pedalo\\\\Documents\\\\GithubCodes\\\\PAN-bot-detection\\\\pan19-author-profiling-training-2019-01-28\\nGram.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(ngram_features), ngram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nGram(tweet, model):\n",
    "    tweet = [tweet]\n",
    "    tweet_ngram = model.transform(tweet)\n",
    "    return tweet_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character flooding\n",
    "eg looooove, floooood\n",
    "Find words that have consecutive characters appearing in them more than 2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CharacterFlooding(tweet):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tweet_words = tknzr.tokenize(tweet)\n",
    "    floodings = 0\n",
    "    regexpURL = re.compile(r'https?:\\/\\/.[^\\s]*|www\\.[^\\s]*')\n",
    "    for word in tweet_words:\n",
    "        if not regexpURL.search(word) and len(re.findall(r'(\\w)\\1{2,}',word)) > 0: \n",
    "            floodings += 1\n",
    "    return floodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Capital Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NumberOfCapitalLetters(tweet):\n",
    "    now, words = NumberOfWords(tweet)\n",
    "    nocl = 0\n",
    "    for word in words:\n",
    "        nocl += sum(1 for c in word if c.isupper())\n",
    "    avg_nocl = nocl / now\n",
    "    return nocl, avg_nocl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NumberOfEmoticons(tweet):\n",
    "    emojis = [i for i in tweet if i in emoji.UNICODE_EMOJI]\n",
    "    return len(emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vacation', 'driving', 'artifact', 'car', 'carparts'} {'vacation', 'driving', 'artifact', 'car', 'carparts'} {'artifact'}\n",
      "Number of topics:  133\n",
      "['Tops', 'act', 'adjectives_for_people', 'animal', 'april_fool', 'art', 'artifact', 'astronomy', 'attribute', 'baseball', 'bathroom', 'beach', 'big', 'biomes', 'birds', 'birthday', 'boat', 'bodies_of_water', 'body', 'buildings', 'camping', 'car', 'carnival', 'carparts', 'castle', 'cats', 'change', 'christmas', 'circus', 'clothes', 'cognition', 'colors', 'communication', 'competition', 'computer', 'constitution', 'consumption', 'contact', 'container', 'cooking', 'cooking_tools', 'country', 'creation', 'dance', 'dentist', 'desserts', 'doctor', 'dogs', 'driving', 'election', 'emotion', 'emotions', 'energy', 'event', 'fall', 'family', 'farm', 'feeling', 'fish', 'flowers', 'food', 'foodweb', 'fruit', 'furniture', 'geography', 'grammar', 'group', 'happiness', 'happy', 'house', 'housing', 'insect', 'jobs', 'kitchen', 'land_forms', 'languages', 'leaders', 'legal', 'location', 'mammal', 'many', 'math', 'measurement', 'metals', 'military', 'money', 'motion', 'motive', 'music_theory', 'musical_instruments', 'mythical_beasts', 'negative_words', 'new_year', 'object', 'ocean', 'office', 'people', 'perception', 'person', 'phenomenon', 'pirate', 'plant', 'plants', 'positive_words', 'possession', 'postal', 'process', 'quantity', 'relation', 'reptiles', 'restaurant', 'roadways', 'rocks', 'rooms', 'school', 'science', 'sciences', 'shape', 'social', 'state', 'stative', 'substance', 'time', 'tree', 'vacation', 'valentine', 'vegetables', 'virtues', 'water', 'weapons', 'weather', 'winter', 'yard']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "SEMCAT_word_concept_dict = pickle.load(open('C:/Users/Vanda/Documents/PAN19/data/pan19-author-profiling-training-2019-01-28/kb/SEMCAT2018_word_concept_dict.p', 'rb'), encoding='utf-8')\n",
    "semcor_word_concept_dict = pickle.load(open('C:/Users/Vanda/Documents/PAN19/data/pan19-author-profiling-training-2019-01-28/kb/semcor_noun_verb.supersenses.en_word_concept_dict.p', 'rb'), encoding='utf-8')\n",
    "SEMCAT_concepts = set([item for sublist in SEMCAT_word_concept_dict.values() for item in sublist])\n",
    "semcor_concepts = set([item for sublist in semcor_word_concept_dict.values() for item in sublist])\n",
    "# merge\n",
    "wordTopicsDict = SEMCAT_word_concept_dict\n",
    "for word, concept_set in semcor_word_concept_dict.items():\n",
    "    wordTopicsDict[word] = wordTopicsDict[word].union(concept_set)\n",
    "topics = SEMCAT_concepts.union(semcor_concepts)\n",
    "topicNames = sorted(list(topics), key=lambda t: t)\n",
    "print(wordTopicsDict['car'], SEMCAT_word_concept_dict['car'], semcor_word_concept_dict['car'])\n",
    "print(\"Number of topics: \", len(topics))\n",
    "print(topicNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in progress (probably using SEMCAT database)\n",
    "def Topics(tweet):\n",
    "    topicsFrequency = {t:0 for t in topics}\n",
    "    tweet = spacy_nlp(tweet)\n",
    "    lemmas = [token.lemma_ for token in tweet]\n",
    "    for token in tweet:\n",
    "        lemma = token.lemma_\n",
    "        topicSet = wordTopicsDict.get(lemma, set())\n",
    "        for t in topicSet:\n",
    "            topicsFrequency[t] += 1\n",
    "    topicsFrequency = OrderedDict(sorted(topicsFrequency.items(), key=lambda e: e[0]))\n",
    "    topicsValues = list(topicsFrequency.values())\n",
    "    return topicsValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NumberOfURLs(tweet):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tweet_words = tknzr.tokenize(tweet)\n",
    "    regexpURL = re.compile(r'https?:\\/\\/.[^\\s]*|www\\.[^\\s]*')\n",
    "    urlNumber = 0\n",
    "    for word in tweet_words:\n",
    "        if regexpURL.search(word): \n",
    "            urlNumber += 1\n",
    "    return urlNumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Repeated Words\n",
    "Compute:\n",
    "- number of tokens repeated more (or equal) than *k* times \n",
    "- maximum number of repetition (of a single token) (>=*k*)\n",
    "\n",
    "For example (k=3) in the tweet: *\"Hairy cats like other cats that are not hairy. However, hairy dogs like cats that are not hairy.\"*\n",
    "- the tokens repeated more (or equal) than 3 times are *hairy* and *cats*, so the number of tokens repeated more than 3 times is 2\n",
    "- the token *hairy* is repeated most of the time: 4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordsRepeated(tweet, k=3):\n",
    "\n",
    "    frequency = defaultdict(int)\n",
    "    tweet = spacy_nlp(tweet)\n",
    "    for token in tweet:\n",
    "        if token.lemma_ not in spacy_stopwords:\n",
    "            frequency[token.lemma_] += 1\n",
    "    maxAppearance = 0\n",
    "    numberOfTokensRepeated = 0\n",
    "    if len(frequency.items()) > 0:\n",
    "        maxAppearance = max(frequency.items(), key=operator.itemgetter(1))[1]\n",
    "        numberOfTokensRepeated = sum(1 for (lemma, freq) in frequency.items() if freq >= k)\n",
    "    return maxAppearance, numberOfTokensRepeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_pos_tags = [\"NO_TAG\", \"ADJ\", \"ADP\", \"ADV\",\"AUX\", \"CONJ\",\"CCONJ\",\"DET\",\n",
    "                      \"INTJ\",\"NOUN\",\"NUM\",\"PART\",\"PRON\",\"PROPN\",\"PUNCT\",\"SCONJ\",\"SYM\",\n",
    "                      \"VERB\",\"X\",\"EOL\",\"SPACE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def POSTags(tweet):\n",
    "    tweet = spacy_nlp(tweet)\n",
    "#     pos_list = []\n",
    "    c = Counter()\n",
    "    c.update({x:0 for x in all_pos_tags})\n",
    "    pos_list = [token.pos_ for token in tweet]\n",
    "    assert len(set(pos_list).difference(set(all_pos_tags))) == 0\n",
    "    c.update(pos_list)\n",
    "    c = OrderedDict(sorted(c.items(), key=lambda e: e[0]))\n",
    "    return list(c.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NumberOfSentences(tweet):\n",
    "    sentences = sent_tokenize(tweet)\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Words\n",
    "using nltk's TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NumberOfWords(tweet):\n",
    "    tknzr = TweetTokenizer()\n",
    "#     words = word_tokenize(tweet)\n",
    "    tweet_words = tknzr.tokenize(tweet)\n",
    "    return len(tweet_words), tweet_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability Score\n",
    "using Fleschâ€“Kincaid readability tests (higher score means the tweet is more easy to read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Readability(tweet):\n",
    "    totalSentences = float(NumberOfSentences(tweet))\n",
    "    totalTweetWords, tweetWords = NumberOfWords(tweet)\n",
    "    totalTweetWords = float(totalTweetWords)\n",
    "    regexpURL = re.compile(r'https?:\\/\\/.[^\\s]*|www\\.[^\\s]*')\n",
    "    totalSyllables = 0.0\n",
    "    for word in tweetWords:\n",
    "        if not regexpURL.search(word):\n",
    "            hyphenated = PYPHEN_DIC.inserted(word)\n",
    "            syllables = hyphenated.count(\"-\") + 1 - hyphenated.count(\"--\")\n",
    "            totalSyllables += syllables\n",
    "#         else:\n",
    "#             totalTweetWords -= 1.0\n",
    "    if totalSentences > 0 and totalTweetWords > 0:\n",
    "        score = 206.835 - 1.015 * (totalTweetWords/totalSentences) - 84.6 * (totalSyllables/totalTweetWords)\n",
    "    else:\n",
    "        print(\"Readability issue\")\n",
    "        score = 0.0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "['avg_ADJ', 'avg_ADP', 'avg_ADV', 'avg_AUX', 'avg_CCONJ', 'avg_CONJ', 'avg_DET', 'avg_EOL', 'avg_INTJ', 'avg_NOUN', 'avg_NO_TAG', 'avg_NUM', 'avg_PART', 'avg_PRON', 'avg_PROPN', 'avg_PUNCT', 'avg_SCONJ', 'avg_SPACE', 'avg_SYM', 'avg_Tops', 'avg_VERB', 'avg_X', 'avg_act', 'avg_adjectives_for_people', 'avg_animal', 'avg_april_fool', 'avg_art', 'avg_artifact', 'avg_astronomy', 'avg_attribute', 'avg_baseball', 'avg_bathroom', 'avg_beach', 'avg_big', 'avg_biomes', 'avg_birds', 'avg_birthday', 'avg_boat', 'avg_bodies_of_water', 'avg_body', 'avg_buildings', 'avg_camping', 'avg_car', 'avg_carnival', 'avg_carparts', 'avg_castle', 'avg_cats', 'avg_change', 'avg_christmas', 'avg_circus', 'avg_clothes', 'avg_cognition', 'avg_colors', 'avg_communication', 'avg_competition', 'avg_computer', 'avg_constitution', 'avg_consumption', 'avg_contact', 'avg_container', 'avg_cooking', 'avg_cooking_tools', 'avg_country', 'avg_creation', 'avg_dance', 'avg_dentist', 'avg_desserts', 'avg_doctor', 'avg_dogs', 'avg_driving', 'avg_election', 'avg_emotion', 'avg_emotions', 'avg_energy', 'avg_event', 'avg_fall', 'avg_family', 'avg_farm', 'avg_feeling', 'avg_fish', 'avg_flowers', 'avg_food', 'avg_foodweb', 'avg_fruit', 'avg_furniture', 'avg_geography', 'avg_grammar', 'avg_group', 'avg_happiness', 'avg_happy', 'avg_house', 'avg_housing', 'avg_insect', 'avg_jobs', 'avg_kitchen', 'avg_land_forms', 'avg_languages', 'avg_leaders', 'avg_legal', 'avg_location', 'avg_mammal', 'avg_many', 'avg_math', 'avg_maxWordAppearancePerTweet', 'avg_measurement', 'avg_metals', 'avg_military', 'avg_money', 'avg_motion', 'avg_motive', 'avg_music_theory', 'avg_musical_instruments', 'avg_mythical_beasts', 'avg_negative_words', 'avg_new_year', 'avg_noURL', 'avg_nocf', 'avg_noclPerWord', 'avg_noe', 'avg_nos', 'avg_now', 'avg_nowr', 'avg_object', 'avg_ocean', 'avg_office', 'avg_people', 'avg_perception', 'avg_person', 'avg_phenomenon', 'avg_pirate', 'avg_plant', 'avg_plants', 'avg_positive_words', 'avg_possession', 'avg_postal', 'avg_process', 'avg_quantity', 'avg_readabilityScore', 'avg_relation', 'avg_reptiles', 'avg_restaurant', 'avg_roadways', 'avg_rocks', 'avg_rooms', 'avg_school', 'avg_science', 'avg_sciences', 'avg_shape', 'avg_social', 'avg_state', 'avg_stative', 'avg_substance', 'avg_time', 'avg_tree', 'avg_vacation', 'avg_valentine', 'avg_vegetables', 'avg_virtues', 'avg_water', 'avg_weapons', 'avg_weather', 'avg_winter', 'avg_yard', 'maxWordAppearancePerProfile']\n"
     ]
    }
   ],
   "source": [
    "fNames = ['nos', 'now', 'readabilityScore', 'noclPerWord', 'noe', 'nocf', 'noURL', 'maxWordAppearancePerTweet', 'nowr']\n",
    "fNames.extend(all_pos_tags)\n",
    "fNames.extend(topicNames)\n",
    "fNames = ['avg_'+f for f in fNames]\n",
    "fNames.append('maxWordAppearancePerProfile')\n",
    "fNames = sorted(fNames)\n",
    "print(len(fNames))\n",
    "print(fNames)\n",
    "fNames_dir = 'C:/Users/Vanda/Documents/PAN19/data/pan19-author-profiling-training-2019-01-28/en_features/'\n",
    "if not os.path.exists(fNames_dir):\n",
    "    os.makedirs(fNames_dir)\n",
    "pickle.dump(fNames, open(fNames_dir+'feature_names.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute features for each tweet\n",
    "ngrams are not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetFeatures(tweet):\n",
    "    features = {}\n",
    "    features['nos'] = NumberOfSentences(tweet)\n",
    "    features['now'], words = NumberOfWords(tweet)\n",
    "    features['readabilityScore'] = Readability(tweet)\n",
    "    nocl, features['noclPerWord'] = NumberOfCapitalLetters(tweet)\n",
    "    features['noe'] = NumberOfEmoticons(tweet)\n",
    "    features['nocf'] = CharacterFlooding(tweet)\n",
    "    features['noURL'] = NumberOfURLs(tweet)\n",
    "    features['maxWordAppearancePerTweet'], features['nowr'] = wordsRepeated(tweet)\n",
    "#     ngram = nGram(tweet, ngram_model)\n",
    "    pos = POSTags(tweet)\n",
    "    posTags = sorted(all_pos_tags)\n",
    "    for i in range(len(posTags)):\n",
    "        features[posTags[i]] = pos[i]\n",
    "    topicValues = Topics(tweet)\n",
    "    for i in range(len(topicNames)):\n",
    "        features[topicNames[i]] = topicValues[i]\n",
    "#     features = [nos, now, readabilityScore, avg_nocl, noe, nocf]\n",
    "#     ngramFeatures = nGram(tweet, ngram_model)\n",
    "#     for i in range(ngramFeatures.shape[1]):\n",
    "#         name = ngram_features[i]\n",
    "#         feat = ngramFeatures[0,i]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute average of features for each Twitter profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cummulateFeatures(profileFeatures):\n",
    "    denom = len(profileFeatures)\n",
    "    cummulatedFeatures = defaultdict(float)\n",
    "    for tweetFeature in profileFeatures:\n",
    "        for featureName in tweetFeature.keys():\n",
    "            cummulatedFeatures[('avg_'+featureName)] += tweetFeature[featureName]\n",
    "    for k, v in cummulatedFeatures.items():\n",
    "        cummulatedFeatures[k] = cummulatedFeatures[k]/denom\n",
    "    cummulatedFeatures = OrderedDict(sorted(cummulatedFeatures.items(), key=lambda e: e[0]))\n",
    "    featureNames = list(cummulatedFeatures.keys())\n",
    "    features = list(cummulatedFeatures.values())\n",
    "    return features, featureNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeProfileFeatures(directory, profileID, profileFeatures, Truth, TweetGenerator):\n",
    "    profileName = Truth[0][profileID]\n",
    "    mode = roleDict[profileName]\n",
    "    features, featureNames = cummulateFeatures(profileFeatures)\n",
    "#     features, featureNames = additionalProfileFeatures(TweetGenerator,  features, featureNames)\n",
    "    features = [str(f) for f in features]\n",
    "    features = '\\t'.join(features)\n",
    "    with codecs.open(directory+'/'+mode+'_features.txt', \"a\", \"utf-8-sig\") as text_file:\n",
    "        text_file.write(profileName+'\\t'+features+'\\t'+Truth[1][profileID]+'\\t'+Truth[2][profileID]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max number of word repetitions for a profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxWordRepetitionsPerProfile(TweetGenerator):\n",
    "    tknzr = TweetTokenizer()\n",
    "    frequency = defaultdict(int)\n",
    "    for tweet in TweetGenerator:\n",
    "        tweet = tweet['data']\n",
    "        tweet = spacy_nlp(tweet)\n",
    "        for token in tweet:\n",
    "            if token.lemma_ not in spacy_stopwords:\n",
    "                frequency[token.lemma_] += 1\n",
    "    maxAppearance = 0\n",
    "    if len(frequency.items()) > 0:\n",
    "        maxAppearance = max(frequency.items(), key=operator.itemgetter(1))[1]\n",
    "    return maxAppearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def additionalProfileFeatures(TweetGenerator, features, featureNames):\n",
    "    maxWordAppearance = maxWordRepetitionsPerProfile(TweetGenerator)\n",
    "    features.append(maxWordAppearance)\n",
    "    featureNames.append('maxWordAppearancePerProfile')\n",
    "    return features, featureNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadProfile(input_file):    \n",
    "    Profile = ET.parse(input_file)\n",
    "    ProfileRoot = Profile.getroot()\n",
    "    Profile_attr = ProfileRoot.attrib\n",
    "    for tweet in Profile.iter('document'):        \n",
    "        tweet_dict = Profile_attr.copy()\n",
    "        tweet_dict.update(tweet.attrib)\n",
    "        tweet_dict['data'] = tweet.text\n",
    "        yield tweet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ProcessFolder(root_directory, input_directory, output_directory):    \n",
    "    #Create output directory (if it does not exist yet)\n",
    "    profile_directory = root_directory+'/'+output_directory+'/'+'profile'\n",
    "    tweet_directory = root_directory+'/'+output_directory+'/'+'tweet'\n",
    "    if os.path.exists(root_directory+'/'+output_directory):\n",
    "        shutil.rmtree(root_directory+'/'+output_directory)\n",
    "    os.mkdir(root_directory+'/'+output_directory)\n",
    "    os.mkdir(profile_directory)\n",
    "    os.mkdir(tweet_directory)\n",
    "    #Read labels (and file names)\n",
    "    Truth = pd.read_csv(root_directory+'/'+input_directory+'/truth.txt', sep=\":::\", header=None, engine='python')\n",
    "    #Iterate over all user names, and process the corresponding file names\n",
    "#     for i in range(0,Truth.shape[0]):\n",
    "    for i in range(0,2):        \n",
    "        #Open text file for output   \n",
    "        print(Truth[0][i])\n",
    "        profileFeatures = []\n",
    "        TweetGenerator = LoadProfile(root_directory+'/'+input_directory+'/'+Truth[0][i]+'.xml')\n",
    "        with codecs.open(tweet_directory+'/'+Truth[0][i]+'.txt', \"w\", \"utf-8-sig\") as text_file: \n",
    "            for tweet in TweetGenerator:\n",
    "                tweetFeatures = GetFeatures(tweet['data'])\n",
    "                profileFeatures.append(tweetFeatures)\n",
    "                tweetFeatures = [str(f) for f in tweetFeatures.values()]\n",
    "                tweetFeatures = '\\t'.join(tweetFeatures)\n",
    "                text_file.write(tweetFeatures + '\\n')   \n",
    "        TweetGenerator = LoadProfile(root_directory+'/'+input_directory+'/'+Truth[0][i]+'.xml')\n",
    "        writeProfileFeatures(profile_directory, i, profileFeatures, Truth, TweetGenerator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProcessFolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-99563d90cf7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mProcessFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\pedalo\\\\Documents\\\\GithubCodes\\\\PAN-bot-detection\\\\pan19-author-profiling-training-2019-01-28'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'en_features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ProcessFolder' is not defined"
     ]
    }
   ],
   "source": [
    "ProcessFolder('C:\\\\Users\\\\pedalo\\\\Documents\\\\GithubCodes\\\\PAN-bot-detection\\\\pan19-author-profiling-training-2019-01-28', 'en', 'en_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
