{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "from wordsegment import load, segment\n",
    "load()\n",
    "from string import punctuation\n",
    "import re\n",
    "import codecs\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "#For creating directories\n",
    "import os\n",
    "\n",
    "#For emojis\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function for loading twitter profile (i.e. load all 100 tweets from an xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadProfile(input_file):    \n",
    "    Profile = ET.parse(input_file)\n",
    "    ProfileRoot = Profile.getroot()\n",
    "    Profile_attr = ProfileRoot.attrib\n",
    "    for tweet in Profile.iter('document'):        \n",
    "        tweet_dict = Profile_attr.copy()\n",
    "        tweet_dict.update(tweet.attrib)\n",
    "        tweet_dict['data'] = tweet.text\n",
    "        yield tweet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove links from the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveURL(tweet):\n",
    "    tweet = re.sub('https?:\\/\\/.[^\\s]*', '', tweet)\n",
    "    #Some URLs still start with www.\n",
    "    tweet = re.sub('www\\.[^\\s]*', '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It would not be necessary to have these functions as standalone,\n",
    "#But I figure that if we do it like so, we can easily amend them with feature extraction\n",
    "def RemoveEmoji(tweet):\n",
    "    tweet = ''.join([i for i in tweet if i not in emoji.UNICODE_EMOJI]) \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stupid character errors (&amp; and whatever else I find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveThrash(tweet):\n",
    "    tweet = re.sub('&amp;', '&', tweet)\n",
    "    tweet = re.sub('&#39;', '\\'', tweet)\n",
    "    tweet = re.sub('&quot;', '\"', tweet)\n",
    "    tweet = re.sub('…', ' ', tweet)\n",
    "    tweet = re.sub('“', '\"', tweet)\n",
    "    tweet = re.sub('”', '\"', tweet)\n",
    "    #There is apparently a character that looks very much like an apostrophe, and is used like an apostrophe, but it is not an apostrophe\n",
    "    tweet = re.sub('’', '\\'', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove @ symbols from mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is handled separately from the punctuation, so we can also try and store the mentions\n",
    "def RemoveAt(tweet):\n",
    "    tweet = re.sub('@', '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove hashtag symbols, and split the hashtags into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveHash(tweet):\n",
    "    #First remove # characters that are followed by whitespaces (they are not true whitespaces)    \n",
    "    tweet = re.sub('#[\\s]+', ' ', tweet)    \n",
    "    #then replace hashtags with tokenized versions\n",
    "    while('#' in tweet):\n",
    "        #print(tweet)\n",
    "        hashtag = re.search('#[^\\s]*', tweet).group(0)                \n",
    "        segmented_hashtag = segment(hashtag)        \n",
    "        hashtag_part = ' '.join(segmented_hashtag)        \n",
    "        tweet = re.sub('#[^\\s]*', hashtag_part, tweet, 1)        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemovePunctuation(tweet):\n",
    "    #Before removing ', let us try to change every '0 to 200 and every 'x to 19x\n",
    "    tweet = re.sub(r'\\'([0-1])', r'20\\1', tweet)\n",
    "    tweet = re.sub(r'\\'([2-9])', r'19\\1', tweet)\n",
    "    tweet = ''.join([i for i in tweet if i not in punctuation])\n",
    "    #After removing the punctuation, we may have to partition again (so for example Sales/Development does not become Sales Development)\n",
    "    #load()\n",
    "    #tweet = segment(tweet)    \n",
    "    #print(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanData(tweet):    \n",
    "    #print('NOISY:')\n",
    "    #print(tweet)\n",
    "    tweet = RemoveURL(tweet)\n",
    "    tweet = RemoveEmoji(tweet)\n",
    "    tweet = RemoveThrash(tweet)\n",
    "    tweet = RemoveAt(tweet)\n",
    "    tweet = RemoveHash(tweet)\n",
    "    tweet = RemovePunctuation(tweet)\n",
    "    #In the end replace all whitespaces with just one space\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #print('CLEAN:')    \n",
    "    #print(tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function: get twitter data contained in folder_in and call the cleaning function on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFolder(root_directory, input_directory, output_directory):    \n",
    "    #Create output directory (if it does not exist yet)\n",
    "    if (not os.path.isdir(root_directory+'/'+output_directory)):\n",
    "        os.mkdir(root_directory+'/'+output_directory)\n",
    "    #Read labels (and file names)\n",
    "    Truth = pd.read_csv(root_directory+'/'+input_directory+'/truth.txt', sep=\":::\", header=None, engine='python')\n",
    "    #Iterate over all user names, and process the corresponding file names\n",
    "    for i in range(0,Truth.shape[0]):\n",
    "    #for i in range(0,2):        \n",
    "        #Open text file for output        \n",
    "        with codecs.open(root_directory+'/'+output_directory+'/'+Truth[0][i]+'.txt', \"w\", \"utf-8-sig\") as text_file:        \n",
    "            #Load the tweets from the current xml to the Current_data array\n",
    "            TweetGenerator = LoadProfile(root_directory+'/'+input_directory+'/'+Truth[0][i]+'.xml')\n",
    "            for tweet in TweetGenerator:                                                        \n",
    "                clean_tweet = CleanData(tweet['data'])                \n",
    "                text_file.write(clean_tweet + '\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try out the CleanData function\n",
    "ProcessFolder('../pan19-author-profiling-training-2019-01-28/','en','en_clean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "306px",
    "left": "1548px",
    "right": "20px",
    "top": "120px",
    "width": "312px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
