{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyphen in c:\\users\\vanda\\anaconda3\\lib\\site-packages (0.9.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\vanda\\anaconda3\\lib\\site-packages (3.2.5)\n",
      "Requirement already satisfied: six in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from nltk) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyphen\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.0.0/es_core_news_md-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.0.0/es_core_news_md-2.0.0.tar.gz (98.5MB)\n",
      "Requirement already satisfied (use --upgrade to upgrade): es-core-news-md==2.0.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.0.0/es_core_news_md-2.0.0.tar.gz in c:\\users\\vanda\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: spacy>=2.0.0a18 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from es-core-news-md==2.0.0) (2.0.3)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (4.4.3)\n",
      "Requirement already satisfied: pathlib in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (1.0.1)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (0.28.0)\n",
      "Requirement already satisfied: msgpack-numpy in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (0.4.1)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (1.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (1.11.0)\n",
      "Requirement already satisfied: ujson>=1.35 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (1.35)\n",
      "Requirement already satisfied: regex==2017.4.5 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (2017.4.5)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (6.10.1)\n",
      "Requirement already satisfied: msgpack-python in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (0.4.8)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (1.15.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (2.18.4)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (0.2.7.1)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (1.31.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from spacy>=2.0.0a18->es-core-news-md==2.0.0) (0.9.6)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from ftfy<5.0.0,>=4.4.2->spacy>=2.0.0a18->es-core-news-md==2.0.0) (0.1.7)\n",
      "Requirement already satisfied: html5lib in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from ftfy<5.0.0,>=4.4.2->spacy>=2.0.0a18->es-core-news-md==2.0.0) (0.9999999)\n",
      "Requirement already satisfied: termcolor in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from thinc<6.11.0,>=6.10.1->spacy>=2.0.0a18->es-core-news-md==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from thinc<6.11.0,>=6.10.1->spacy>=2.0.0a18->es-core-news-md==2.0.0) (4.28.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from thinc<6.11.0,>=6.10.1->spacy>=2.0.0a18->es-core-news-md==2.0.0) (1.10.11)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from thinc<6.11.0,>=6.10.1->spacy>=2.0.0a18->es-core-news-md==2.0.0) (0.8.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->es-core-news-md==2.0.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->es-core-news-md==2.0.0) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->es-core-news-md==2.0.0) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->es-core-news-md==2.0.0) (2018.1.18)\n",
      "Requirement already satisfied: pyreadline>=1.7.1 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from dill<0.3,>=0.2->spacy>=2.0.0a18->es-core-news-md==2.0.0) (2.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in c:\\users\\vanda\\anaconda3\\lib\\site-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy>=2.0.0a18->es-core-news-md==2.0.0) (0.8.2)\n",
      "Building wheels for collected packages: es-core-news-md\n",
      "  Building wheel for es-core-news-md (setup.py): started\n",
      "  Building wheel for es-core-news-md (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Vanda\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-93d82sp9\\wheels\\5a\\2c\\3e\\f54b751b98219db5864c24e25f032433969db931582f7dd23a\n",
      "Successfully built es-core-news-md\n",
      "\n",
      "    Error: Couldn't link model to 'es_core_news_md'\n",
      "    Creating a symlink in spacy/data failed. Make sure you have the required\n",
      "    permissions and try re-running the command as admin, or use a\n",
      "    virtualenv. You can still import the model as a module and call its\n",
      "    load() method, or create the symlink manually.\n",
      "\n",
      "    C:\\Users\\Vanda\\Anaconda3\\lib\\site-packages\\es_core_news_md -->\n",
      "    C:\\Users\\Vanda\\Anaconda3\\lib\\site-packages\\spacy\\data\\es_core_news_md\n",
      "\n",
      "\n",
      "    Download successful\n",
      "    Creating a shortcut link for 'en' didn't work (maybe you don't have\n",
      "    admin permissions?), but you can still load the model via its full\n",
      "    package name:\n",
      "\n",
      "    nlp = spacy.load('es_core_news_md')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import emoji\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter, OrderedDict, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('es_core_news_md')\n",
    "spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vanda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pyphen\n",
    "PYPHEN_DIC = pyphen.Pyphen(lang='es')\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character flooding\n",
    "eg looooove, floooood\n",
    "Find words that have consecutive characters appearing in them more than 2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CharacterFlooding(tweet):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tweet_words = tknzr.tokenize(tweet)\n",
    "    floodings = 0\n",
    "    regexpURL = re.compile(r'https?:\\/\\/.[^\\s]*|www\\.[^\\s]*')\n",
    "    for word in tweet_words:\n",
    "        if not regexpURL.search(word) and len(re.findall(r'(\\w)\\1{2,}',word)) > 0: \n",
    "            floodings += 1\n",
    "    return floodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Capital Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NumberOfCapitalLetters(tweet):\n",
    "    now, words = NumberOfWords(tweet)\n",
    "    nocl = 0\n",
    "    for word in words:\n",
    "        nocl += sum(1 for c in word if c.isupper())\n",
    "    avg_nocl = nocl / now\n",
    "    return nocl, avg_nocl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NumberOfEmoticons(tweet):\n",
    "    emojis = [i for i in tweet if i in emoji.UNICODE_EMOJI]\n",
    "    return len(emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "SEMCAT_word_concept_dict = pickle.load(open('C:/Users/Vanda/Documents/PAN19/data/pan19-author-profiling-training-2019-01-28/kb/SEMCAT2018_word_concept_dict.p', 'rb'), encoding='utf-8')\n",
    "semcor_word_concept_dict = pickle.load(open('C:/Users/Vanda/Documents/PAN19/data/pan19-author-profiling-training-2019-01-28/kb/semcor_noun_verb.supersenses.en_word_concept_dict.p', 'rb'), encoding='utf-8')\n",
    "SEMCAT_concepts = set([item for sublist in SEMCAT_word_concept_dict.values() for item in sublist])\n",
    "semcor_concepts = set([item for sublist in semcor_word_concept_dict.values() for item in sublist])\n",
    "# merge data\n",
    "wordTopicsDict = SEMCAT_word_concept_dict\n",
    "for word, concept_set in semcor_word_concept_dict.items():\n",
    "    wordTopicsDict[word] = wordTopicsDict[word].union(concept_set)\n",
    "# words_file = open(\"words_to_translate.txt\", \"w\")\n",
    "# for word, topics in wordTopicsDict.items():\n",
    "#     words_file.write(word+'\\n')\n",
    "# words_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_words = open('sp_words.txt', 'r', encoding='utf-8')\n",
    "en_words = list(wordTopicsDict.keys())\n",
    "en_es_dict = {}\n",
    "i = 0\n",
    "for line in es_words:\n",
    "    es_word = (line.split())[0]\n",
    "    es_word = es_word.lower() # using lower case translations\n",
    "    en_word = en_words[i]\n",
    "    en_es_dict[en_word] = es_word\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics:  133\n",
      "Topics:  ['Tops', 'act', 'adjectives_for_people', 'animal', 'april_fool', 'art', 'artifact', 'astronomy', 'attribute', 'baseball', 'bathroom', 'beach', 'big', 'biomes', 'birds', 'birthday', 'boat', 'bodies_of_water', 'body', 'buildings', 'camping', 'car', 'carnival', 'carparts', 'castle', 'cats', 'change', 'christmas', 'circus', 'clothes', 'cognition', 'colors', 'communication', 'competition', 'computer', 'constitution', 'consumption', 'contact', 'container', 'cooking', 'cooking_tools', 'country', 'creation', 'dance', 'dentist', 'desserts', 'doctor', 'dogs', 'driving', 'election', 'emotion', 'emotions', 'energy', 'event', 'fall', 'family', 'farm', 'feeling', 'fish', 'flowers', 'food', 'foodweb', 'fruit', 'furniture', 'geography', 'grammar', 'group', 'happiness', 'happy', 'house', 'housing', 'insect', 'jobs', 'kitchen', 'land_forms', 'languages', 'leaders', 'legal', 'location', 'mammal', 'many', 'math', 'measurement', 'metals', 'military', 'money', 'motion', 'motive', 'music_theory', 'musical_instruments', 'mythical_beasts', 'negative_words', 'new_year', 'object', 'ocean', 'office', 'people', 'perception', 'person', 'phenomenon', 'pirate', 'plant', 'plants', 'positive_words', 'possession', 'postal', 'process', 'quantity', 'relation', 'reptiles', 'restaurant', 'roadways', 'rocks', 'rooms', 'school', 'science', 'sciences', 'shape', 'social', 'state', 'stative', 'substance', 'time', 'tree', 'vacation', 'valentine', 'vegetables', 'virtues', 'water', 'weapons', 'weather', 'winter', 'yard']\n"
     ]
    }
   ],
   "source": [
    "# use spanish translation of words alongside english words (some Spanish tweets might contain English words)\n",
    "import copy\n",
    "es_en_wordTopicsDict = copy.deepcopy(wordTopicsDict)\n",
    "\n",
    "for word, topics in wordTopicsDict.items():\n",
    "    w = word\n",
    "    translation = en_es_dict[word]\n",
    "    if translation not in es_en_wordTopicsDict.keys():\n",
    "        es_en_wordTopicsDict[translation] = topics\n",
    "    else:\n",
    "        es_en_wordTopicsDict[translation].union(topics)\n",
    "topics = SEMCAT_concepts.union(semcor_concepts)\n",
    "topicNames = sorted(list(topics), key=lambda t: t)\n",
    "print(\"Number of topics: \", len(topics))\n",
    "print(\"Topics: \", topicNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Topics(tweet):\n",
    "    topicsFrequency = {t:0 for t in topics}\n",
    "    tweet = spacy_nlp(tweet)\n",
    "    lemmas = [token.lemma_ for token in tweet]\n",
    "    for token in tweet:\n",
    "        lemma = token.lemma_\n",
    "        topicSet = wordTopicsDict.get(lemma, set())\n",
    "        for t in topicSet:\n",
    "            topicsFrequency[t] += 1\n",
    "    topicsFrequency = OrderedDict(sorted(topicsFrequency.items(), key=lambda e: e[0]))\n",
    "    topicsValues = list(topicsFrequency.values())\n",
    "    return topicsValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NumberOfURLs(tweet):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tweet_words = tknzr.tokenize(tweet)\n",
    "    regexpURL = re.compile(r'https?:\\/\\/.[^\\s]*|www\\.[^\\s]*')\n",
    "    urlNumber = 0\n",
    "    for word in tweet_words:\n",
    "        if regexpURL.search(word): \n",
    "            urlNumber += 1\n",
    "    return urlNumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Repeated Words\n",
    "Compute:\n",
    "- number of tokens repeated more (or equal) than *k* times \n",
    "- maximum number of repetition (of a single token) (>=*k*)\n",
    "\n",
    "For example (k=3) in the tweet: *\"Hairy cats like other cats that are not hairy. However, hairy dogs like cats that are not hairy.\"*\n",
    "- the tokens repeated more (or equal) than 3 times are *hairy* and *cats*, so the number of tokens repeated more than 3 times is 2\n",
    "- the token *hairy* is repeated most of the time: 4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordsRepeated(tweet, k=3):\n",
    "    frequency = defaultdict(int)\n",
    "    tweet = spacy_nlp(tweet)\n",
    "    for token in tweet:\n",
    "        if token.lemma_ not in spacy_stopwords:\n",
    "            frequency[token.lemma_] += 1\n",
    "    maxAppearance = 0\n",
    "    numberOfTokensRepeated = 0\n",
    "    if len(frequency.items()) > 0:\n",
    "        maxAppearance = max(frequency.items(), key=operator.itemgetter(1))[1]\n",
    "        numberOfTokensRepeated = sum(1 for (lemma, freq) in frequency.items() if freq >= k)\n",
    "    return maxAppearance, numberOfTokensRepeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Number of POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_pos_tags = [\"NO_TAG\", \"ADJ\", \"ADP\", \"ADV\",\"AUX\", \"CONJ\",\"CCONJ\",\"DET\",\n",
    "                      \"INTJ\",\"NOUN\",\"NUM\",\"PART\",\"PRON\",\"PROPN\",\"PUNCT\",\"SCONJ\",\"SYM\",\n",
    "                      \"VERB\",\"X\",\"EOL\",\"SPACE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def POSTags(tweet):\n",
    "    tweet = spacy_nlp(tweet)\n",
    "#     pos_list = []\n",
    "    c = Counter()\n",
    "    c.update({x:0 for x in all_pos_tags})\n",
    "    pos_list = [token.pos_ for token in tweet]\n",
    "    assert len(set(pos_list).difference(set(all_pos_tags))) == 0\n",
    "    c.update(pos_list)\n",
    "    c = OrderedDict(sorted(c.items(), key=lambda e: e[0]))\n",
    "    return list(c.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Number of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NumberOfSentences(tweet):\n",
    "    sentences = sent_tokenize(tweet)\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NumberOfWords(tweet):\n",
    "    tknzr = TweetTokenizer()\n",
    "#     words = word_tokenize(tweet)\n",
    "    tweet_words = tknzr.tokenize(tweet)\n",
    "    return len(tweet_words), tweet_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability Score\n",
    "using Flesch–Kincaid readability tests (higher score means the tweet is more easy to read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Readability(tweet):\n",
    "    totalSentences = float(NumberOfSentences(tweet))\n",
    "    totalTweetWords, tweetWords = NumberOfWords(tweet)\n",
    "    totalTweetWords = float(totalTweetWords)\n",
    "    regexpURL = re.compile(r'https?:\\/\\/.[^\\s]*|www\\.[^\\s]*')\n",
    "    totalSyllables = 0.0\n",
    "    for word in tweetWords:\n",
    "        if not regexpURL.search(word):\n",
    "            hyphenated = PYPHEN_DIC.inserted(word)\n",
    "            syllables = hyphenated.count(\"-\") + 1 - hyphenated.count(\"--\")\n",
    "            totalSyllables += syllables\n",
    "#         else:\n",
    "#             totalTweetWords -= 1.0\n",
    "    if totalSentences > 0 and totalTweetWords > 0:\n",
    "        score = 206.835 - 1.015 * (totalTweetWords/totalSentences) - 84.6 * (totalSyllables/totalTweetWords)\n",
    "    else:\n",
    "        print(\"Readability issue\")\n",
    "        score = 0.0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "['avg_ADJ', 'avg_ADP', 'avg_ADV', 'avg_AUX', 'avg_CCONJ', 'avg_CONJ', 'avg_DET', 'avg_EOL', 'avg_INTJ', 'avg_NOUN', 'avg_NO_TAG', 'avg_NUM', 'avg_PART', 'avg_PRON', 'avg_PROPN', 'avg_PUNCT', 'avg_SCONJ', 'avg_SPACE', 'avg_SYM', 'avg_Tops', 'avg_VERB', 'avg_X', 'avg_act', 'avg_adjectives_for_people', 'avg_animal', 'avg_april_fool', 'avg_art', 'avg_artifact', 'avg_astronomy', 'avg_attribute', 'avg_baseball', 'avg_bathroom', 'avg_beach', 'avg_big', 'avg_biomes', 'avg_birds', 'avg_birthday', 'avg_boat', 'avg_bodies_of_water', 'avg_body', 'avg_buildings', 'avg_camping', 'avg_car', 'avg_carnival', 'avg_carparts', 'avg_castle', 'avg_cats', 'avg_change', 'avg_christmas', 'avg_circus', 'avg_clothes', 'avg_cognition', 'avg_colors', 'avg_communication', 'avg_competition', 'avg_computer', 'avg_constitution', 'avg_consumption', 'avg_contact', 'avg_container', 'avg_cooking', 'avg_cooking_tools', 'avg_country', 'avg_creation', 'avg_dance', 'avg_dentist', 'avg_desserts', 'avg_doctor', 'avg_dogs', 'avg_driving', 'avg_election', 'avg_emotion', 'avg_emotions', 'avg_energy', 'avg_event', 'avg_fall', 'avg_family', 'avg_farm', 'avg_feeling', 'avg_fish', 'avg_flowers', 'avg_food', 'avg_foodweb', 'avg_fruit', 'avg_furniture', 'avg_geography', 'avg_grammar', 'avg_group', 'avg_happiness', 'avg_happy', 'avg_house', 'avg_housing', 'avg_insect', 'avg_jobs', 'avg_kitchen', 'avg_land_forms', 'avg_languages', 'avg_leaders', 'avg_legal', 'avg_location', 'avg_mammal', 'avg_many', 'avg_math', 'avg_maxWordAppearancePerTweet', 'avg_measurement', 'avg_metals', 'avg_military', 'avg_money', 'avg_motion', 'avg_motive', 'avg_music_theory', 'avg_musical_instruments', 'avg_mythical_beasts', 'avg_negative_words', 'avg_new_year', 'avg_noURL', 'avg_nocf', 'avg_noclPerWord', 'avg_noe', 'avg_nos', 'avg_now', 'avg_nowr', 'avg_object', 'avg_ocean', 'avg_office', 'avg_people', 'avg_perception', 'avg_person', 'avg_phenomenon', 'avg_pirate', 'avg_plant', 'avg_plants', 'avg_positive_words', 'avg_possession', 'avg_postal', 'avg_process', 'avg_quantity', 'avg_readabilityScore', 'avg_relation', 'avg_reptiles', 'avg_restaurant', 'avg_roadways', 'avg_rocks', 'avg_rooms', 'avg_school', 'avg_science', 'avg_sciences', 'avg_shape', 'avg_social', 'avg_state', 'avg_stative', 'avg_substance', 'avg_time', 'avg_tree', 'avg_vacation', 'avg_valentine', 'avg_vegetables', 'avg_virtues', 'avg_water', 'avg_weapons', 'avg_weather', 'avg_winter', 'avg_yard', 'maxWordAppearancePerProfile']\n"
     ]
    }
   ],
   "source": [
    "fNames = ['nos', 'now', 'readabilityScore', 'noclPerWord', 'noe', 'nocf', 'noURL', 'maxWordAppearancePerTweet', 'nowr']\n",
    "fNames.extend(all_pos_tags)\n",
    "fNames.extend(topicNames)\n",
    "fNames = ['avg_'+f for f in fNames]\n",
    "fNames.append('maxWordAppearancePerProfile')\n",
    "fNames = sorted(fNames)\n",
    "print(len(fNames))\n",
    "print(fNames)\n",
    "fNames_dir = 'C:/Users/Vanda/Documents/PAN19/data/pan19-author-profiling-training-2019-01-28/en_features/'\n",
    "if not os.path.exists(fNames_dir):\n",
    "    os.makedirs(fNames_dir)\n",
    "pickle.dump(fNames, open(fNames_dir+'feature_names.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute features for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetFeatures(tweet):\n",
    "    features = {}\n",
    "    features['nos'] = NumberOfSentences(tweet)\n",
    "    features['now'], words = NumberOfWords(tweet)\n",
    "    features['readabilityScore'] = Readability(tweet)\n",
    "    nocl, features['noclPerWord'] = NumberOfCapitalLetters(tweet)\n",
    "    features['noe'] = NumberOfEmoticons(tweet)\n",
    "    features['nocf'] = CharacterFlooding(tweet)\n",
    "    features['noURL'] = NumberOfURLs(tweet)\n",
    "    features['maxWordAppearancePerTweet'], features['nowr'] = wordsRepeated(tweet)\n",
    "\n",
    "    pos = POSTags(tweet)\n",
    "    posTags = sorted(all_pos_tags)\n",
    "    for i in range(len(posTags)):\n",
    "        features[posTags[i]] = pos[i]\n",
    "    topicValues = Topics(tweet)\n",
    "    for i in range(len(topicNames)):\n",
    "        features[topicNames[i]] = topicValues[i]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute average of features for each Twitter profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cummulateFeatures(profileFeatures):\n",
    "    denom = len(profileFeatures)\n",
    "    cummulatedFeatures = defaultdict(float)\n",
    "    for tweetFeature in profileFeatures:\n",
    "        for featureName in tweetFeature.keys():\n",
    "            cummulatedFeatures[('avg_'+featureName)] += tweetFeature[featureName]\n",
    "    for k, v in cummulatedFeatures.items():\n",
    "        cummulatedFeatures[k] = cummulatedFeatures[k]/denom\n",
    "    cummulatedFeatures = OrderedDict(sorted(cummulatedFeatures.items(), key=lambda e: e[0]))\n",
    "    featureNames = list(cummulatedFeatures.keys())\n",
    "    features = list(cummulatedFeatures.values())\n",
    "    return features, featureNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeProfileFeatures(directory, profileID, profileFeatures, Truth, TweetGenerator):\n",
    "    profileName = Truth[0][profileID]\n",
    "    features, featureNames = cummulateFeatures(profileFeatures)\n",
    "#     features, featureNames = additionalProfileFeatures(TweetGenerator,  features, featureNames)\n",
    "    features = [str(f) for f in features]\n",
    "    features = '\\t'.join(features)\n",
    "    with codecs.open(directory+'/features.txt', \"a\", \"utf-8-sig\") as text_file:\n",
    "        text_file.write(profileName+'\\t'+features+'\\t'+Truth[1][profileID]+'\\t'+Truth[2][profileID]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max number of word repetitions for a profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxWordRepetitionsPerProfile(TweetGenerator):\n",
    "    tknzr = TweetTokenizer()\n",
    "    frequency = defaultdict(int)\n",
    "    for tweet in TweetGenerator:\n",
    "        tweet = tweet['data']\n",
    "        tweet = spacy_nlp(tweet)\n",
    "        for token in tweet:\n",
    "            if token.lemma_ not in spacy_stopwords:\n",
    "                frequency[token.lemma_] += 1\n",
    "    maxAppearance = 0\n",
    "    if len(frequency.items()) > 0:\n",
    "        maxAppearance = max(frequency.items(), key=operator.itemgetter(1))[1]\n",
    "    return maxAppearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def additionalProfileFeatures(TweetGenerator, features, featureNames):\n",
    "    maxWordAppearance = maxWordRepetitionsPerProfile(TweetGenerator)\n",
    "    features.append(maxWordAppearance)\n",
    "    featureNames.append('maxWordAppearancePerProfile')\n",
    "    return features, featureNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Profile & Process Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadProfile(input_file):    \n",
    "    Profile = ET.parse(input_file)\n",
    "    ProfileRoot = Profile.getroot()\n",
    "    Profile_attr = ProfileRoot.attrib\n",
    "    for tweet in Profile.iter('document'):        \n",
    "        tweet_dict = Profile_attr.copy()\n",
    "        tweet_dict.update(tweet.attrib)\n",
    "        tweet_dict['data'] = tweet.text\n",
    "        yield tweet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ProcessFolder(root_directory, input_directory, output_directory):    \n",
    "    #Create output directory (if it does not exist yet)\n",
    "    profile_directory = root_directory+'/'+output_directory+'/'+'profile'\n",
    "    tweet_directory = root_directory+'/'+output_directory+'/'+'tweet'\n",
    "    if os.path.exists(root_directory+'/'+output_directory):\n",
    "        shutil.rmtree(root_directory+'/'+output_directory)\n",
    "    os.mkdir(root_directory+'/'+output_directory)\n",
    "    os.mkdir(profile_directory)\n",
    "    os.mkdir(tweet_directory)\n",
    "    #Read labels (and file names)\n",
    "    Truth = pd.read_csv(root_directory+'/'+input_directory+'/truth.txt', sep=\":::\", header=None, engine='python')\n",
    "    #Iterate over all user names, and process the corresponding file names\n",
    "    for i in range(0,Truth.shape[0]):\n",
    "#     for i in range(0,2):        \n",
    "        #Open text file for output   \n",
    "        print(Truth[0][i])\n",
    "        profileFeatures = []\n",
    "        TweetGenerator = LoadProfile(root_directory+'/'+input_directory+'/'+Truth[0][i]+'.xml')\n",
    "        with codecs.open(tweet_directory+'/'+Truth[0][i]+'.txt', \"w\", \"utf-8-sig\") as text_file: \n",
    "            for tweet in TweetGenerator:\n",
    "                tweetFeatures = GetFeatures(tweet['data'])\n",
    "                profileFeatures.append(tweetFeatures)\n",
    "                tweetFeatures = [str(f) for f in tweetFeatures.values()]\n",
    "                tweetFeatures = '\\t'.join(tweetFeatures)\n",
    "                text_file.write(tweetFeatures + '\\n')   \n",
    "        TweetGenerator = LoadProfile(root_directory+'/'+input_directory+'/'+Truth[0][i]+'.xml')\n",
    "        writeProfileFeatures(profile_directory, i, profileFeatures, Truth, TweetGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acfdc9e43ed5efa10a903453261a3c12\n",
      "40adbb05f96fdd96f767b5967458faf1\n",
      "4aa2fb302140ec35cc6bc8a0d7d35f6\n",
      "58db587d884d22afefbcd37aa26af458\n",
      "dd9494d1bff7fa477cc03fea5294a510\n",
      "6ccf89c7b178b36d95d241c1c5f3da23\n",
      "b8ab53c54215915da53006d415aadc35\n",
      "9616ecddb159ad7ad99421903ac05499\n",
      "8b6826222306fd4694afbd709a1cb52c\n",
      "c75d850ee43e714116bf7c7b6a4f7793\n",
      "d4f83ae6c529a0379bbf48fa02a5bbda\n",
      "23fb30684841d6a5444efd32ad75e240\n",
      "d9bb54f64ab9f077398c858e6ccee805\n",
      "888fb276582537e6cbb354a4925bbbbf\n",
      "4f8b1d79f22eaaa45b6832c80e7d1b6b\n",
      "e316ea877ed9602e1df08df6a3064ce0\n",
      "7f73cce03dfbcdd6af4a61249c737a7c\n",
      "af2d03f136310f46566c9e744741857a\n",
      "5deafa3dd19c9c88628ec36c21dd8d4d\n",
      "f180c6c1d4a11b4df97610f88ba856ef\n",
      "17e41f62a2a464e816391675705dbbfb\n",
      "c25555f6b5c8ec8257ca11c34e6a35da\n",
      "b5ac8bf7cb4513ed69810235716d7211\n",
      "6d5dfde2e326bb581217e50059307f0\n",
      "da17f2e8fcc0a11c6ae3963aea7c7f7d\n",
      "489c96c091bff6308a80e82a85766cd9\n",
      "94b820618fd3e05791e9b34ed8765a31\n",
      "22623bbd527fe2bdee9a009a8539c0e8\n",
      "c26253914fb0c6dbcabdf8a763a571dc\n",
      "92df748ec8d5f55f3d6bc25d7a9e3125\n",
      "61b76989cd710eec6a6b25858e746247\n",
      "5ac701546987ab1f7df3664d1a6ab1be\n",
      "5c8f6a413970ec69f57c5fb53fa7b75e\n",
      "7aed00ff4f181f51b12e8c426632415b\n",
      "d6c7d130e26aac0c7fbddc765f77608\n",
      "76416f55125bb5d297710dacc3b7a4f0\n",
      "d51d9551cb52e704ef72b3f984ac5f60\n",
      "fe98e0f75df809d4f3438b2686c05f3b\n",
      "345fbc3e5d8cade852378b97bb94bb4e\n",
      "a19090f4c748e642dbfb12ed6c00430a\n",
      "cc70592c45ba77b7e3471136fe522e8f\n",
      "16e2c730f0a79791a8e811ce0273f7cb\n",
      "ba5e626cd5903edf0c70b72336680fc6\n",
      "a7d0165981fcb8baa34fc1e01ea9d71\n",
      "a73552a4eeab48e4386c2aa1773a797\n",
      "fd6bd76a63e6b26d23c2bcf8a07ba7f0\n",
      "de0ee58c63421b45801605d2ca16ccdf\n",
      "bcb9bb81b8e082299383fa7ac7970744\n",
      "d883382d916309314ff4b5ba96810a0c\n",
      "afa0095a54917ae435ec8d8e8bbff0a7\n",
      "35d1f8210ca425a33bf659a60b2a0523\n",
      "11a0be6704fb8e025fab59d1f38fe449\n",
      "8082a75bf62955b1a7c7165199c7fe34\n",
      "7a8112a96ce904906e27a36c37413b00\n",
      "e6bed8c1433d22a6c21f9a792d80e8af\n",
      "6a28da18acf4eb33b0ef2fe1bcc14308\n",
      "e3251297fec8d7db0cde79b6d242467a\n",
      "a10301e1b2fd0f2d70579d10cc816353\n",
      "feae20989b266844fc75ec10730cedbe\n",
      "8a7c9ad1b519ef2f43763a57af2e21b4\n",
      "ce00969e0150feb2a05f89a393056c99\n",
      "e04907852cb7936e87e68ac326fe4cab\n",
      "ed4414352a72519a7e6ad8ac6bd489c0\n",
      "b26afdd8a081322e0ea2672874a5f629\n",
      "6160d35caaedc8c81745e7cbe94c1ad4\n",
      "b6dac9ffa9aa09737288f1cbd2256abd\n",
      "7c198d2d43f4a015854412477528ac37\n",
      "91c6579e71ed9c17735b29d016c77177\n",
      "9cc33906945cfbb9de4381ae478b08bc\n",
      "c32444b66ae0a9927a02c332cde78ca4\n",
      "352b5324b48dcc0a559e3662b8384301\n",
      "a2e20c65e3d6c8aa4769105cb0675e4\n",
      "532ef519b36acfadd9eb8ebbc4f3821d\n",
      "c00f0087e48301f8f85008532a82afde\n",
      "74c5a3feae42dd7006bb091f036bffed\n",
      "84e87d322da75830bafa16efd0a97114\n",
      "369442695201b4800ec2902f4ddb89cd\n",
      "6703ad79e1aaca497be15c347c1efec1\n",
      "b6a183d61f61c59d896080e2cec4da0e\n",
      "beaf3c1fd0b1dc1691c3e0a557f15b39\n",
      "1ea0bfdf2ab674809f8454df5f36f3c9\n",
      "7d861abc5a66d8250fc26610005d1057\n",
      "a2d68de03cba2dd190f93f4ed82dabac\n",
      "cc26676bee86bc976a61612c10a46c5f\n",
      "d84765ece3f4c1cafb3b8e7287969325\n",
      "47ba25443ed49a65258db0adaf52c37a\n",
      "ad1ce518a84a224beef560b7a6a4ba2\n",
      "2fec859e759f4e4d08de78e53e8a20d6\n",
      "a13ca2d019c391fb8386e243c1a0fa33\n",
      "e8df521cf2afab5b1cdee1063bab60b3\n",
      "b919597e81c765bd80deeb81fb0ed67c\n",
      "ca2b6f36777779637fc6784ecd4d262c\n",
      "bb6b54a6e4f284ba83638340483928f0\n",
      "c681837036ad368c84ee4ff521c98a48\n",
      "6ef9aafc5fc34fd1b5e8d34986b6e984\n",
      "212464b99cefca641802f6551c4d7909\n",
      "c1ad7c893e69e1b897ea08a02544133a\n",
      "aca76fbfcd110a37d4762485e0d2ea32\n",
      "c4f25974eb33ea1d268f3153c716a4f1\n",
      "ae4e5253f2b85c0d393e17777be88453\n",
      "5f552de3add298e831534be4d09fbdc8\n",
      "78b7fd1ae213865daccfcf775e082c6\n",
      "ddd6f20ccf5c6e599295cbef880ec328\n",
      "339ac01d51940c7f3c96001163ea8af4\n",
      "761da7c8cf1929d45298f5b5c2ed2d79\n",
      "b9c7f28d53684939083e36b756fe5035\n",
      "3dbd8f3489264e2d49fcf19cd1ac1147\n",
      "717231311a04d6a5a6d1e0d604d21fed\n",
      "54c4ec8bb26fdc80dd2961634c5091b6\n",
      "ab07f92d9ee6c0d238da6679559ed53b\n",
      "ba90ef072338da156d3ae5be2da19b2f\n",
      "c80bd70dda18789cafc087c808c120b4\n",
      "72e86386004adec6d238e26a582ebc2d\n",
      "feb2370e38d0be8c013d7e4e403cdf39\n",
      "551ce6ac5d7bdf8f71e66d4926c504ac\n",
      "ad6768948300a0b136b1fcefc82aa0e8\n",
      "451b13817dba0f4c7c92f7e7b07c462e\n",
      "8dbff5ae38c2fa195cd714f6816d81d7\n",
      "e6bd4c8c4a5564e1467c024a91fe137e\n",
      "c3a00ff6c5c16a37b0d7660862235661\n",
      "c34a46800f72d685552324b86d2c574d\n",
      "188ea141c0435e19cbf7b1f022627f80\n",
      "f5be079d2f17c68b16cbb55bdc44910f\n",
      "13f7eb5594e503d1401065e8c7e0f267\n",
      "1b9682368f10574097adaf0b9b550ea4\n",
      "aadb73ef7d80dc053d19aaa3ff16c070\n",
      "403c2c287090980cb14ce4221bc82e26\n",
      "eb4e73e763469da9e9e903a7b96639c1\n",
      "824e0e38f906a332769d64f893839597\n",
      "acc5006b0935615ac57c5a173b4cbb4a\n",
      "e373eaeade9f45eca2114a9aadbbbc1e\n",
      "e5552ba7b152620200ff4295b9454ff2\n",
      "651426a17ec0e1e813cc97e1b5f7c9d1\n",
      "9c457e028f2bc2e17939ea6a768a52bf\n",
      "73454c28c355302569d072c30f4ebe30\n",
      "dc9f6b328c1512d788ee4aa64b35fd01\n",
      "c24f69a9a5619058f33f87ff27cda370\n",
      "d749ae972ac7334ad3c1ff39b19f894c\n",
      "29cb5e6449ab47a4298c4767dfd7cd77\n",
      "b45083465d47dfab479e624d640b91ab\n",
      "986fb6adb8268ce4713b34080a334f64\n",
      "67d1117648b2809d8408e04399e7ac7a\n"
     ]
    }
   ],
   "source": [
    "ProcessFolder('C:\\\\Users\\\\Vanda\\\\Documents\\\\PAN19\\\\data\\\\pan19-author-profiling-training-2019-01-28', 'es', 'es_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
